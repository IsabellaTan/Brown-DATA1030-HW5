{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem set 5\n",
    "\n",
    "## Name: [TODO]\n",
    "\n",
    "## Link to your PS5 github repo: [TODO]\n",
    "\n",
    "### Problem 0 \n",
    "\n",
    "-2 points for every missing green OK sign. \n",
    "\n",
    "Make sure you are in the DATA1030 environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from packaging.version import parse as Version\n",
    "from platform import python_version\n",
    "\n",
    "OK = '\\x1b[42m[ OK ]\\x1b[0m'\n",
    "FAIL = \"\\x1b[41m[FAIL]\\x1b[0m\"\n",
    "\n",
    "try:\n",
    "    import importlib\n",
    "except ImportError:\n",
    "    print(FAIL, \"Python version 3.12.10 is required,\"\n",
    "                \" but %s is installed.\" % sys.version)\n",
    "\n",
    "def import_version(pkg, min_ver, fail_msg=\"\"):\n",
    "    mod = None\n",
    "    try:\n",
    "        mod = importlib.import_module(pkg)\n",
    "        if pkg in {'PIL'}:\n",
    "            ver = mod.VERSION\n",
    "        else:\n",
    "            ver = mod.__version__\n",
    "        if Version(ver) == Version(min_ver):\n",
    "            print(OK, \"%s version %s is installed.\"\n",
    "                  % (lib, min_ver))\n",
    "        else:\n",
    "            print(FAIL, \"%s version %s is required, but %s installed.\"\n",
    "                  % (lib, min_ver, ver))    \n",
    "    except ImportError:\n",
    "        print(FAIL, '%s not installed. %s' % (pkg, fail_msg))\n",
    "    return mod\n",
    "\n",
    "\n",
    "# first check the python version\n",
    "pyversion = Version(python_version())\n",
    "\n",
    "if pyversion >= Version(\"3.12.10\"):\n",
    "    print(OK, \"Python version is %s\" % pyversion)\n",
    "elif pyversion < Version(\"3.12.10\"):\n",
    "    print(FAIL, \"Python version 3.12.10 is required,\"\n",
    "                \" but %s is installed.\" % pyversion)\n",
    "else:\n",
    "    print(FAIL, \"Unknown Python version: %s\" % pyversion)\n",
    "\n",
    "    \n",
    "print()\n",
    "requirements = {'numpy': \"2.2.5\", 'matplotlib': \"3.10.1\",'sklearn': \"1.6.1\", \n",
    "                'pandas': \"2.2.3\",'xgboost': \"3.0.0\", 'shap': \"0.47.2\", \n",
    "                'polars': \"1.27.1\", 'seaborn': \"0.13.2\"}\n",
    "\n",
    "# now the dependencies\n",
    "for lib, required_version in list(requirements.items()):\n",
    "    import_version(lib, required_version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1\n",
    "\n",
    "We will investigate the importance of preprocessing your dataset in this exercise. You will work with the unprocessed diabetes dataset in problem 1a, and you will preprocess it in 1b. You'll train a regression model in both problems and you'll compare the feature coefficients. This exercise is a preparation for one of the last topics we will cover in class: interpretability.\n",
    "\n",
    "The coefficients of a linear model (the $w$'s) can be used to measure the importance of features if and only if all features have the same mean and standard deviation (even the one-hot encoded features!). Here is why:\n",
    "\n",
    "The feature coefficients are determined by two things:\n",
    "- the importance of the feautre\n",
    "    - more important features have larger $w$'s\n",
    "- the order of magnitude of the feature relative to the target variable\n",
    "    - if a feature tends to be orders of magnitudes larger than the target variable, the feature's weight will be small in order to bring the feature to the same order of magnitude as the target variable \n",
    "    - similarly, if a feature tends to be orders of magnitudes smaller than the target variable, the feature's weight will be large in order to bring the feature to the same order of magnitude as the target variable \n",
    "\n",
    "This is an ambiguity since a feature could have a large $w$ because either it is important and/or because the feature values tend to be much smaller than the target variable.\n",
    "\n",
    "The only way to remove this ambiguity is to standardize all features. If all features have the same mean and the same standard deviation (most commonly 0 mean and 1 standard deviation when you use the StandardScaler), all features have the same order of magnitudes. Then the feature coefficient is purely determined by the importance of the feature and it can be used to rank features based on how important they are.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1a (10 points)\n",
    "\n",
    "First, read in the dataset into a pandas dataframe using the tab delimited file linked at [this page](https://www4.stat.ncsu.edu/~boos/var.select/diabetes.html). Loop through 10 different random states. Split the dataset into training and test sets (70-30 ratios). We won't tune the regularization parameter, so we don't need a validation set for this exercise. Do not preprocess the sets. Train a linear regression model with Ridge regularization with alpha = 1. Use RMSE as your evaluation metric and print out the mean and standard deviation of the training and test scores. For each random state, save the test score and the model's coefficients for each feature.\n",
    "\n",
    "Once you loop through the 10 random states, prepare one plot which shows the mean and standard deviation of each feature's coefficient. Make sure to have proper x and y ticks and labels, as well as a title.\n",
    "\n",
    "Hint: use plt.errorbar() or something similar for the figure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1b (10 points)\n",
    "\n",
    "Now let's repeat what you did in 1a but this time preprocess the sets with the standard scaler.\n",
    "\n",
    "Notice how the coefficients change as a result of preprocessing. \n",
    "\n",
    "Answer the following questions:\n",
    "\n",
    "Q1: Do you see a statistically significant change in the test score?\n",
    "\n",
    "Q2: How would you order the features based on the coefficients for feature importance? Explain your answer in a few sentences.\n",
    "\n",
    "Q3: Print out names of the top 3 most important features!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**your answers here**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2\n",
    "\n",
    "In class we have seen both l1 (Lasso) and l2 (Ridge) regression. These are two basic ways to perform regularization. In the following problem, we will explore the **elastic net**, a third regularization technique that combines both l1 and l2 penalties. We will use this in a classification context. \n",
    "\n",
    "The basic idea of the elastic net is that the cost function in regression becomes\n",
    "\n",
    "$L(\\theta) = \\frac{1}{n}\\sum_{i=1}^{n}[(\\theta_0 + \\sum_{j=1}^{m} \\theta_j  x_{ij}- y_i)^2] + \\color{red}{ \\alpha \\rho \\sum_{j=0}^{m}|\\theta_j|} + \\color{blue}{ \\alpha (1-\\rho) \\sqrt{\\sum_{j=0}^{m} \\theta_j^2}}$,\n",
    "\n",
    "and the cost function in classification is\n",
    "\n",
    "$L(\\theta) = - \\frac{1}{N}\\sum_{i=1}^{n} [y_i\\ln(\\frac{1}{1+e^{-\\theta_0 + \\sum_{j=1}^{m} \\theta_j  x_{ij}}}) + (1-y_i)\\ln(1-\\frac{1}{1+e^{-\\theta_0 + \\sum_{j=1}^{m} \\theta_j  x_{ij}}}))] + \\color{red}{ \\alpha \\rho \\sum_{j=0}^{m}|\\theta_j|} + \\color{blue}{ \\alpha (1-\\rho) \\sqrt{\\sum_{j=0}^{m} \\theta_j^2}}$,\n",
    "\n",
    "\n",
    "where $\\alpha$ is the regularization parameter and $\\rho$ is the l1 ratio (how much weight we assign to the l1 term over the l2 term in the cost function). Basically, an elastic net uses the weighted sum of the l1 and l2 regularization terms. The weight of the l1 term is $\\rho$ and the weight of the l2 term is $(1-\\rho)$ where $\\rho$ is between 0 and 1.\n",
    "\n",
    "You can read more about the elastic net [here](https://scikit-learn.org/stable/modules/linear_model.html#elastic-net).\n",
    "\n",
    "Note that there are a large number of regularization techniques available in sklearn, the complete list of linear models are described [here](https://scikit-learn.org/stable/modules/linear_model.html).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2a (3 points)\n",
    "\n",
    "Load the training and validation sets from train.csv and val.csv from the `data` folder. \n",
    "\n",
    "Run a logistic regression model without regularization on the data and print the accuracy score of the validation set. Use the 'saga' solver. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2b (7 points)\n",
    "\n",
    "Perform l1 regularization on the data. \n",
    "\n",
    "The value of the alpha should contain 21 uniformly spaced values in log from 1e-2 to 1e2. \n",
    "\n",
    "Again, use the 'saga' solver and if you see a converge warning, fix it without ignoring the warning. \n",
    "\n",
    "Plot the train and validation accuracy scores. \n",
    "\n",
    "Print the best validation accuracy score and the corresponding alpha values.  If multiple alpha values give you euqlly good validation scores, print out all the alpha values, not just one.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2c (1 point)\n",
    "\n",
    "Perform l2 regularization on the data. The alpha values and all the other instructions are the same as in 2b."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2d (10 points)\n",
    "\n",
    "Let's train an elastic net now! The elastic net has two parameters: alpha and rho (the l1 ratio). The l1 ratio should be linearly spaced between 0 and 1 with 21 values in between, while alpha should be spaced as outlined above. (1 point) Use the 'saga' solver. The reason we use the saga solver everywhere in Problem 2 is that it is the only solver in LogisticRegression that works with an elastic net. \n",
    "\n",
    "Calculate the train and validation accuracy scores for all combinations of alpha and rho. \n",
    "\n",
    "Print the best validation score and the corresponding alpha and rho values. \n",
    "\n",
    "Prepare heatmaps to show the train and validation scores. Make sure that the data range covered by the two heatmaps are the same so you can easily compare the two heatmaps and you can identify the high bias and high variance regions. Label the plot and add a colorbar. Make the x and y ticks look pretty. \n",
    "\n",
    "Which of the four approaches gave you the best validation score?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
